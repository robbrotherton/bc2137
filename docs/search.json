[
  {
    "objectID": "index.html#course-details",
    "href": "index.html#course-details",
    "title": "Social Psychology Lab Handbook",
    "section": "Course details",
    "text": "Course details\nPre-requisites: PSYC BC1001 Introduction to Psychology; PSYC BC1101 Statistics; PSYC BC1020 Research Methods\nCo-requisite: PSYC BC2138 Social Psychology\n\nTime, venue & instructor\nSection 001: Wednesday 10:10-1PM, Milbank 410\nSection 002: Wednesday 1:10-4:00PM, Milbank 410\nInstructor: Dr. Rob Brotherton (rbrother@barnard.edu)\nOffice hour: Friday 10-11AM, Milbank 415M"
  },
  {
    "objectID": "index.html#course-overview",
    "href": "index.html#course-overview",
    "title": "Social Psychology Lab Handbook",
    "section": "Course overview",
    "text": "Course overview\nThis lab will usually be taken concurrently with BC2138 Social lecture. It will expand upon some of the theoretical, methodological and analytic issues introduced there, as well as giving you the opportunity to explore topics of your choosing from within (or beyond) those covered in the lectures in greater depth through hands-on experience with research design and data analysis.\nThe semester is broken into 3 projects, each involving an analysis of existing data. The first will involve planning and executing a correlational analysis. The second will involve performing an ANOVA analysis. For the third project, you will design your own analysis. Through these projects you will gain experience in formulating social-psychological research questions; analyzing and visualizing data; and interpreting and communicating your findings. Projects will be undertaken in groups; group members will collaborate on design and analysis. Each project will culminate in an in-class group presentation and submission of a brief individual write up of your project.\n\nClass format & participation\nLabs are substantially more interactive and discussion-based than the traditional lecture format, and depend on everyone’s active participation in class discussions and activity as well as group work focused around the projects. Your active participation across the semester will therefore contribute a substantial portion of your grade.\nIf you have questions, thoughts, or ideas you want to share, feel free to do so at any time (while keeping within the bounds of polite conversation, obviously–don’t interrupt or talk over other people! But do feel free to respond to others without having to raise your hand or wait to be called on). Everyone will get the most out of this lab when the discussion can develop organically and everyone feels free to be part of the conversation if & when they have something to add.\nBeing part of the in-person discussion is one obvious way to participate, but it’s not the only way. Different people have different styles of participation, and the lab is designed to try and accommodate and encourage different approaches. Your level of engagement with your project partners and Prof. Brotherton as you work through your projects is also an important form of participation. You can also participate by coming to office hours.\nAt a minimum (i.e. for a passing grade), I’ll be looking for some form of participation (loosely defined) from you every week. Higher participation grades will be earned through regular, enthusiastic, productive participation (note that quality is more important than quantity).\n\n\nWorkload\nAs a general rule for the amount of time students should expect to commit to classes, the college suggests three hours per week in or outside of class per credit. Since this class is worth 1.5 credits, that corresponds to 4.5 hours per week, split between time in the classroom and time spent completing the associated assignments.\n\n\nFinal Grades\nYour numeric score for the course is a product of your scores for each assignment, weighted as follows:\n\n\n\n\n\n\nWeight (%)\n\n\n\n\nParticipation (over the course of the semester)\n10\n\n\nPresentations\n30\n\n\nReports\n60\n\n\n\n\n\n\n\nFinal grades are determined according to the following boundaries:\nLetter grade:  A+ A  A- B+ B  B- C+ C  C- D  F\nNumeric score: 97 93 90 87 83 80 77 73 70 60 &lt;60"
  },
  {
    "objectID": "index.html#course-policies",
    "href": "index.html#course-policies",
    "title": "Social Psychology Lab Handbook",
    "section": "Course policies",
    "text": "Course policies\n\nAttendance & timeliness\nIn-person attendance of every lab session is expected, and you should expect to stay for the full duration of the lab. Normally it is departmental policy to remove students who miss more than two lab sessions from the course; however, given ongoing revisions to college-wide health-related policies, exceptions may be made. If you are feeling unwell, you should not come to class and notify me of nonattendance before class if possible.\nWhen you are attending, please arrive on time for class. Frequent lateness will impact your participation grade.\n\n\nAssignment deadlines & late policy\nAssignments are listed in the class schedule next to the class in which details about completing the assignment will be provided. The assignment must be completed and submitted before the following class, i.e. requirements for the first presentation slide will be explained in class on 2/14, and the slide must be submitted before the next class on 2/21.\nFor the written project reports, a grade penalty of 5 points will be applied for each day (or part thereof) that an assignment is late (up to a maximum of 6 days; work not submitted before the next lab will receive a score of zero). For example, if your work is A+ quality but is submitted a day-and-a-half late, you will only receive a B+. This policy is intended to incentivize timely submission while easing the stress of genuine emergencies. When things come up that prevent timely submission you can prioritize accordingly, knowing that a small penalty on one assignment for this lab will not tank your final grade.\nLate submission for the presentation slides will not be possible; failure to submit a link to your slides in advance of the presentation will obviously limit your presentation grade.\n\n\nAcademic integrity\nStudents are expected to follow the Barnard Honor Code, available at https://barnard.edu/honor-code.\nNote that while you will collaborate with group members on the design, analysis, and presentation of research projects, you may not collaborate on the written report: each group member must write their own individual reports.\n\n\nAcademic accommodations and general wellness\nIt is always important to recognize the different pressures, burdens, and stressors you may be facing, whether personal, emotional, physical, financial, mental, or academic. The faculty and administration recognize this, and are prepared to provide assistance to students in need. I encourage you to seek advice from your advisor, Dean, the Center for Accessibility Resources & Disability Services (CARDS), or Barnard Health & Wellness as needed. Please let me know of any issues you wish to share with me that you feel are impacting your ability to complete the course to the best of your ability. Though it isn’t always easy, it is better to proactively seek help rather than letting problems build up."
  },
  {
    "objectID": "index.html#class-schedule",
    "href": "index.html#class-schedule",
    "title": "Social Psychology Lab Handbook",
    "section": "Class schedule",
    "text": "Class schedule\n\n\n\n\n\nDate\nTopic\nAssignment*\n\n\n\n\n1/24\nCourse overview\n\n\n\nProject 1: Correlation\n\n\n1/31\nProject planning\n\n\n\n2/7\nData preparation\n\n\n\n2/14\nAnalysis\nPresentation slide\n\n\n2/21\nPresentations\nProject 1 report\n\n\nProject 2: ANOVA\n\n\n2/28\nProject planning\n\n\n\n3/6\nData preparation & analysis\n\n\n\n3/13\nNo class (Spring Break)\n\n\n\n3/20\nVisualization & interpretation\nPresentation slide\n\n\n3/27\nPresentations\nProject 2 report\n\n\nProject 3: Own design\n\n\n4/3\nProject planning\n\n\n\n4/10\nAnalysis\n\n\n\n4/17\nVisualization & interpretation\nPresentation slide\n\n\n4/24\nPresentation & report\nProject 3 report\n\n\n\n* due by the following class"
  },
  {
    "objectID": "lab-2.html#project-overview",
    "href": "lab-2.html#project-overview",
    "title": "Lab 2: Project Planning",
    "section": "Project overview",
    "text": "Project overview\n\nCorrelational designs\nWith this project you will examine a correlation between social psychological constructs using real survey data.\nA correlation refers an association between two things. It is a statement of a statistical relationship–a general tendency, rather than a rigid law. To say that something correlates with something else–for example, satisfaction with ones friendships is correlated with wellbeing or prejudice is correlated with endoresement of stereotypes towards a group–is to say that those things tend to go together. Not everyone who feels great satisfaction in their friendships will report greater wellbeing than anyone with less social satisfaction, but there is some tendency for the two to go together on the whole.\nOf course, these kind of correlations aren’t just facts found lying around in nature; they are empirical findings produced by researchers. All the findings you learn about in the social psychology lecture (and beyond) are the product of research procedures. Researchers decide what psychological constructs they want to investigate; how to measure those constructs; what statistical analyses are appropriate; and what conclusions may be drawn. There are strengths, limitations, and trade offs involved in every decision along the way.\n\n\nStep 1. Examine the data and identify your variables\nThe dataset we will use is from the American National Election Studies (ANES), academic surveys of voters in the United States conducted before and after every presidential election, going back to the 1940s. Specifically, for this project we will use data collected around the 2020 election, since it’s the most recent survey.\nThe kind of correlation analysis you will perform examines whether two constructs are related. The full dataset contains more than 1,000 questions reflecting various constructs. Since this is your first project I am going to constrain your choice. You will investigate how perceived threats are associated with trust. Does distrust of the government and/or other people tend to go together with feeling threatened in some way?\nOne of your variables will be a question assessing perceived threat:\n\nHow worried are you about losing your job in the near future? (V201540)\nSo far as you and your family are concerned, how worried are you about your current financial situation? (V201594)\nHow concerned are you about losing your health insurance in the next year? (V201621)\nHow concerned are you about being able to pay health care expenses for you and your family in the next year? (V201622)\nHow worried do you feel about how things are going in the country? (V201120)\nWhat do you think about the state of the economy these days in the United States? (V201324)\nHow worried are you that the United States will experience a terrorist attack in the near future? (V202358)\n\nYour second variable will be a question about trust:\n\nHow often can you trust the federal government in Washington to do what is right? (V201233)\nHow many of the people running the government are corrupt? (V201236)\nGenerally speaking, how often can you trust other people? (V201237)\n\nTo see exactly how these constructs were measured you will look up the variable IDs (the codes beginning with V in parentheses above) in the Codebook.\n\n\nStep 2. Read relevant research\nReal research doesn’t happen in a vacuum; research plans and expectations should be informed by what has come before. Therefore once you know which variables you will analyze, you will see what other researchers have found about these (or related) constructs.\nA real research project would involve an exhaustive literature review, in which you attempt to find and understand all the research relevant to your question. Since this project is just for practice and our time is limited, you don’t need to read everything; this paper should give you an idea of what has been found:\n\n\n\nSchlipphak, B. (2021). Threat perceptions, blame attribution, and political trust. Journal of Elections, Public Opinion and Parties. https://doi.org/10.1080/17457289.2021.2001474\n\n\n\n\nStep 3. Articulate your design and hypothesis\nBy this point you should be able to state your:\n\nOperational definitions (that is, the specific questions that participants were asked and how they could answer, per the codebook)\nThe constructs that those operational definitions measure (i.e. does the question measure perceived threat in general? Or some more specific form of perceived threat?)\nYour hypothesis\n\nYour hypothesis is a formal statement of your expectation about how your constructs are (or aren’t) associated, and it will be tested quantitatively by calculating a correlation statistic.\nThe main question that your hypothesis addresses is: do you think the two variables will be significantly correlated? That is, will you find an association consistent enough that it doesn’t just seem to be attributable to chance variation in the data?1\nIf you do expect a significant correlation, you should also specify whether you expect it to be positive or negative, and how strong you expect it to be (i.e. weak, moderate, strong; see Appendix B)."
  },
  {
    "objectID": "lab-2.html#footnotes",
    "href": "lab-2.html#footnotes",
    "title": "Lab 2: Project Planning",
    "section": "",
    "text": "Even random data will produce spurious correlations by chance some of the time; see spuriouscorrelations.com↩︎"
  },
  {
    "objectID": "lab-3.html#working-with-data-in-r",
    "href": "lab-3.html#working-with-data-in-r",
    "title": "Lab 3: Data preparation",
    "section": "Working with data in R",
    "text": "Working with data in R\n\nGetting R ready\nIn addition to being a source of high-quality data relevant to social psychology, the ANES dataset is convenient for our purposes because someone went to the trouble of creating an R package which makes working with the data relatively straightforward (not that you won’t still run into issues!): anesr (github.com/jamesmartherus/anesr).\nTo start exploring the data in R, you first need to set up your environment. This means installing the anesr package. Usually packages can be installed from within R using the install.packages() function. However since the anesr package is hosted on GitHub (as opposed to the official R repository of packages), the easiest way to install it is by first installing the devtools package, which has a special function for installing packages from GitHub.\n\ninstall.packages(\"devtools\")\n\ndevtools::install_github(\"jamesmartherus/anesr\")\n\nWe will also use some other packages for data wrangling and analysis. Developers have created a collection of packages for R called the tidyverse to make coding these common tasks easier. The tidyverse can be installed like so:\n\ninstall.packages(\"tidyverse\")\n\nIf you execute those lines of code the packages will be installed on your system. That step only needs to be done once, but you need to ‘activate’ the packages using library() to make their functions and data available each time to start a new R session.\n\nlibrary(anesr)\nlibrary(tidyverse)\n\n\n\nGetting data into R\nGetting data into R often involves reading in a .csv (comma-separated values) spreadsheet file that you downloaded to your computer. Indeed, you could download the ANES data file as a .csv from the ANES website and read it into R that way. However, the anesr package contains the data so you don’t need to download it separately. Instead you can make it available by running this line of code:\n\ndata(timeseries_2020)\n\nWhen you execute the code you won’t see any output, but you should see the name timeseries_2020 appear in your Environment pane. That is now an object in R called a data.frame. You can think of it as a spreadsheet like you’re familiar with from Excel or Google Sheets; a set of columns, one for each variable in the dataset, and a row for each participant’s answers.\nTyping the name of the data.frame and running that line of code will show the first few columns and rows.\n\ntimeseries_2020\n\n# A tibble: 8,280 × 1,381\n   version         V200001 V160001_orig V200002 V200003 V200004 V200005 V200010a\n   &lt;chr&gt;             &lt;dbl&gt;   &lt;hvn_lbll&gt; &lt;hvn_l&gt; &lt;hvn_l&gt; &lt;hvn_l&gt; &lt;hvn_l&gt;    &lt;dbl&gt;\n 1 ANES2020TimeSe…  200015       401318       3       2      -2       0    0.828\n 2 ANES2020TimeSe…  200022       300261       3       2      -2       0    1.09 \n 3 ANES2020TimeSe…  200039       400181       3       2      -2       0    0.672\n 4 ANES2020TimeSe…  200046       300171       3       2      -2       0    0.492\n 5 ANES2020TimeSe…  200053       405145       3       2      -2       1    1.19 \n 6 ANES2020TimeSe…  200060       400374       3       2      -2       0    0.339\n 7 ANES2020TimeSe…  200084       407013       3       2      -2       0    0.525\n 8 ANES2020TimeSe…  200091       407174       3       2      -2       0    0.729\n 9 ANES2020TimeSe…  200107       406264       3       2      -2       0    1.42 \n10 ANES2020TimeSe…  200114       402782       3       2      -2       1    2.56 \n# ℹ 8,270 more rows\n# ℹ 1,373 more variables: V200010b &lt;dbl&gt;, V200010c &lt;dbl&gt;, V200010d &lt;dbl&gt;,\n#   V200011a &lt;dbl&gt;, V200011b &lt;dbl&gt;, V200011c &lt;dbl&gt;, V200011d &lt;dbl&gt;,\n#   V200012a &lt;dbl&gt;, V200012b &lt;dbl&gt;, V200012c &lt;dbl&gt;, V200012d &lt;dbl&gt;,\n#   V200013a &lt;dbl&gt;, V200013b &lt;dbl&gt;, V200013c &lt;dbl&gt;, V200013d &lt;dbl&gt;,\n#   V200014a &lt;dbl&gt;, V200014b &lt;dbl&gt;, V200014c &lt;dbl&gt;, V200014d &lt;dbl&gt;,\n#   V200015a &lt;dbl&gt;, V200015b &lt;dbl&gt;, V200015c &lt;dbl&gt;, V200015d &lt;dbl&gt;, …\n\n\nYou can also click on the name in the Environment pane to view the data in a new tab.\n\n\nSelect your variables\nAs you can see, the data.frame contains a lot of variables; there are 1,381 columns of data, one for each recorded variable. You’ll only need two of those. So the first step is selecting just the variables you need to work with.\nThere are a lot of ways to do this. The simplest would be to make a note of the variable IDs from the codebook and use the select() function.1 This allows us to simply type in variable names separated by commas.\nFor this example code I’ll look at two variables similar to those I had you pick from: trust in news media (In general, how much trust and confidence do you have in the news media when it comes to reporting the news fully, accurately, and fairly?) and perceived threat to the media (How concerned are you that some people in the government today might want to undermine the news media’s ability to serve as a check on governmental power?). Their variable IDs are V201377 and V201376 respectively. Since I’ll probably forget which ID is which, I’ll give the columns more meaningful names as I select them.\n\nmy_data &lt;- timeseries_2020 |&gt; \n  select(trust = V201377, \n         threat = V201376) |&gt; \n  haven::zap_labels()\n\nI did something else there as well: haven::zap_labels(). Even though when you look at the data it’s a bunch of numbers, it’s actually a special format called “haven-labelled”, meaning there is some extra info stored about the numbers behind-the-scenes. That can be useful to have, but it actually interferes with some of the numeric filtering and mutating we need to do momentarily, so the zap_labels() function drops that label information and turns the data into plain old numbers.\nLet’s see what this new data.frame looks like:\n\nmy_data\n\n# A tibble: 8,280 × 2\n   trust threat\n   &lt;dbl&gt;  &lt;dbl&gt;\n 1     1      1\n 2     1      1\n 3     3      5\n 4     4      3\n 5     2      4\n 6     4      4\n 7     2      5\n 8     1      1\n 9     3      2\n10     3      2\n# ℹ 8,270 more rows\n\n\nIt all looks good so far. But if you inspect the data more extensively (click the name in your Environment to open a tab showing the data and scroll down a bit) you’ll notice that there are some negative numbers in the data. You can see all the unique values recorded for a column in the data like so:\n\nunique(my_data$trust)\n\n[1]  1  3  4  2  5 -9 -8\n\n\nThe negatives are from survey codes which record missing data. If you try to calculate an average score with those included it’ll mess up the sums, so we need to do some data cleaning to handle things like that.\n\n\nCleaning the data\nThere are a lot of different ways we could handle this. One way is to filter() the data, retaining only rows which meet certain conditions.2\nThe ANES coding scheme uses negative values for the various kinds of missing or inappropriate data, which makes things simple: only positive values are valid and should be retained.\nTo implement this as a filter(), we can use the if_all() function; i.e., we are going to select some columns and if all the values in those columns meet some condition the row will be retained. To select the columns we can use the everything() function, since the positive-valid/negative-invalid rule is true of every column in our data. The part after the comma, ~ . &gt;= 0, articulates the condition. The ~ prefix is necessary because instead of naming one specific column to refer to its values we use . as a placeholder representing the values in each of the selected columns; the value must be greater than or equal to 0 to be retained.3\n\nmy_data_complete &lt;- my_data |&gt; \n  filter(if_all(everything(), ~ . &gt;= 0))\n\nNotice that the number of rows in the data.frame has changed, because rows that didn’t meet that condition have been dropped.\n\nnrow(my_data)\n\n[1] 8280\n\nnrow(my_data_complete) \n\n[1] 8211\n\n\nAfter filtering to keep only rows with complete data, we’re left with 8,211 valid responses.\n\n\nRecoding values\nNow that we have selected our columns and filtered out missing/invalid responses, the last thing to do is recode values so they all mean what we want them to mean.\nNotice that valid responses for the “undermine the news media” threat item are 1 (not at all concerned) through 5 (extremely concerned). I want higher scores on that question to indicate greater perceived threat, so that’s fine.\nFor the “trust in news media” question, responses are 1 (none) though 5 (a great deal). But I’m thinking of the psychological construct as distrust rather than trust, so I want higher scores to indicate more distrust. The solution is simple: subtract the participant’s answer from 6 (one more than the maximum score) so that a response of 1 becomes a 5 (the most distrust), 2 becomes 4, 3 stays 3, 4 becomes 2 and 5 becomes 1 (the least distrust).\n\nmy_data_complete &lt;- my_data_complete |&gt;\n  mutate(distrust = 6 - trust)\n\nNow I have my two variables, perceived threat to the news media, and (reverse-coded) distrust of the news media, for each of the 8,211 participants with complete data. We’re ready to start exploring the data."
  },
  {
    "objectID": "lab-3.html#start-examining-the-data",
    "href": "lab-3.html#start-examining-the-data",
    "title": "Lab 3: Data preparation",
    "section": "Start examining the data",
    "text": "Start examining the data\n\nDescriptive statistics\nThe most common descriptive statistics are the mean (\\(M\\)) and standard deviation (\\(SD\\)). You should report these for each variable in your analysis.\nThere are many ways of doing this, but for now I’ll just use the mean() and sd() functions. I can refer to a particular column in a data.frame using the $ operator, i.e. my_data$threat and so on.\n\nmean(my_data_complete$threat)\n\n[1] 3.446718\n\nsd(my_data_complete$threat)\n\n[1] 1.363856\n\nmean(my_data_complete$distrust)\n\n[1] 3.54086\n\nsd(my_data_complete$distrust)\n\n[1] 1.208428\n\n\n\n\nVisualizing distributions\nIn addition to reporting the mean and standard deviation, it is useful to visualize the distribution of the data. This can reveal nuances that are not obvious in those single numeric summary values.\nAs with most things, there are a lot of different ways of producing graphs using R. One of the most widely used and powerful is the ggplot2 package.4 The name refers to the idea of the “grammar of graphics”, and it is built around a layering approach. You first specify your data and aesthetics (what should data will go on the X and Y axes), then geometry (do you want data to be represented by points or bars or as a histogram?), any scaling (e.g. what values should be labeled on each axis), and theme elements (how do you want the plot to look generally?). There can be a lot of complexity, but building things up layer by layer, gradually adding and refining elements, is a powerful and satisfying approach.\nHere’s a simple histogram of the threat item. I pipe the data into the ggplot() function, specifying that I want the threat column to be represented as the x aesthetic. Then I add geometry using geom_histogram. That geom function automatically computes bins and counts; here I just specify I want a binwidth of 1, i.e. each column of the histogram will represent one scale point. Note that ggplot layers are added using + rather than the usual |&gt; pipe.\n\nmy_data_complete |&gt; \n  ggplot(aes(x = threat)) +\n  geom_histogram(binwidth = 1)\n\n\n\n\nFigure 1: Histogram of responses to perceived threat to news media question\n\n\n\n\nThe default theme is perfectly serviceable, but you can customize every element. Here I’ll specify a couple of aspects using the theme() function, and I’ll assign it to the name theme_apa. Then I can always add theme_apa as a layer to my plots going forward.\n\ntheme_apa &lt;- theme(\n  panel.background = element_blank(),\n  axis.line = element_line()\n)\n\nI’ll also customize the “breaks” and “labels” on the x-axis (to show the verbal scale responses rather than just numeric codes), and the labels for the x and y axes.\n\nmy_data_complete |&gt; \n  ggplot(aes(x = threat)) +\n  geom_histogram(binwidth = 1, color = \"white\") +\n  scale_x_continuous(breaks = 1:5, \n                     labels = c(\" Not at all\", \"A little\", \"Moderately\", \"Very\", \"Extremely\")) +\n  labs(x = \"How concerned are you that some people in the government today might want to\\nundermine the news media’s ability to serve as a check on governmental power?\",\n       y = \"Number of responses\") +\n  theme_apa\n\n\n\n\nFigure 2: Histogram of responses to perceived threat to news media question\n\n\n\n\nHere’s a histogram of the distrust item.\n\nmy_data_complete |&gt; \n  ggplot(aes(x = distrust)) +\n  geom_histogram(binwidth = 1, color = \"white\") +\n  scale_x_continuous(breaks = 1:5,\n                     labels = c(\"A great deal\", \"A lot\", \"A moderate amount\", \"A little\", \"None\")) +\n  labs(x = \"In general, how much trust and confidence do you have in the news media \\n when it comes to reporting the news fully, accurately, and fairly?\",\n       y = \"Number of responses\") +\n  theme_apa\n\n\n\n\nFigure 3: Histogram of responses to “distrust” question"
  },
  {
    "objectID": "lab-3.html#footnotes",
    "href": "lab-3.html#footnotes",
    "title": "Lab 3: Data preparation",
    "section": "",
    "text": "The select() function, along with filter(), mutate(), across(), everything(), and others that you’ll see in my example code, is part of the tidyverse family of packages (specifically these all come from the dplyr package, but we’ll also use functions from other tidyverse packages like tidyr and ggplot2). There are other ways to do all these things without using tidyverse packages, just relying on what’s referred to as “base” R functions. The tidyverse approach just makes this kind of data manipulation generally easier and makes the code more interpretable. If you’re curious to see how base R and tidyverse functions differ in syntax, a good place to start is https://dplyr.tidyverse.org/articles/base.html.↩︎\nAnother way would be to mutate() the data, changing the invalid response codes into the value NA, R’s special value to indicate missing data. This could be achieved like so:\nmy_data_complete &lt;- my_data |&gt;\n  mutate(across(everything(), ~replace(., . &lt; 0, NA)))\nThat would mutate (i.e. change values) across every column. You can read the second part (after the ~) as “replace the original values (indicated by the placeholder .), where the value is less than zero, with NA.↩︎\nIf the data wasn’t as simple or if we just wanted to be more explicit about things, we could filter based on valid responses for each item. For example, valid reponses to the feeling thermometer item are are anything from 0 to 100; anything else is invalid. Therefore we could write a filter() condition stating that feeling_thermometer (the name of the column) values must be %in% the set of values from 0:100. Likewise for each of the extraversion columns, rows will be retained only if their values are %in% the range 1:7.\nmy_data_complete &lt;- my_data |&gt;\n  filter(trust %in% 1:5,\n         threat %in% 1:5)\n↩︎\nThe ggplot2 package is part of the tidyverse, so because we already ran library(tidyverse) earlier the ggplot2 functions are already available to us. If you needed to, you could always run library(ggplot2) to activate it separately.↩︎"
  },
  {
    "objectID": "lab-4.html#correlation",
    "href": "lab-4.html#correlation",
    "title": "Lab 4: Analysis",
    "section": "Correlation",
    "text": "Correlation\nRunning with my example from last week, I made a data.frame with just the two variables I needed; filtered the data down to complete, valid responses; and recoded the trust item so higher scores indicate greater distrust. To refresh your memory, here’s the setup and data preparation pipeline from start to finish:\n\nlibrary(tidyverse)\nlibrary(anesr)\ndata(timeseries_2020)\n\nmy_data_complete &lt;- timeseries_2020 |&gt;\n  select(trust = V201377, \n         threat = V201376) |&gt; \n  haven::zap_labels() |&gt;\n  filter(if_all(everything(), ~ . &gt;= 0)) |&gt; \n  mutate(distrust = 6 - trust)\n\n\nThe correlation statistic\nThe correlation statistic can be computed with a single line of code, as you’ll see. But it’s important to understand the math happening behind the scenes.\nIf you need to refresh your memory from a past statistics class, refer to the correlation statistic Appendix.\n\n\nComputing a correlation\nThe correlation between two variables can be found using the cor() function.\n\ncor(x = my_data_complete$threat, \n    y = my_data_complete$distrust)\n\n[1] -0.4955168\n\n\nIf you got an answer of NA instead of a number, it is probably because your data has some missing data. You just need to tell cor() to only use data for which both pairs of values are nonmissing:\n\ncor(x = my_data_complete$threat, \n    y = my_data_complete$distrust,\n    use = \"pairwise.complete.obs\")\n\n[1] -0.4955168\n\n\nThe cor.test() function goes further than cor(), giving you the \\(p\\)-value necessary for determining statistical significance1 and some other information about the correlation.\n\ncor.test(x = my_data_complete$threat, \n         y = my_data_complete$distrust)\n\n\n    Pearson's product-moment correlation\n\ndata:  my_data_complete$threat and my_data_complete$distrust\nt = -51.687, df = 8209, p-value &lt; 2.2e-16\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n -0.5116630 -0.4790208\nsample estimates:\n       cor \n-0.4955168 \n\n\n\n\nVisualizing a correlation\nLastly, let’s make a scatterplot visualizing the correlation.\n\nmy_data_complete |&gt; \n  ggplot(aes(x = threat, y = distrust)) +\n  geom_point(position = position_jitter(width = 0.45, height = 0.45, seed = 1),\n             alpha = 0.1) +\n  scale_x_continuous(breaks = 1:5, \n                     labels = c(\" Not at all\", \"A little\", \"Moderately\", \"Very\", \"Extremely\")) +\n  scale_y_continuous(breaks = 1:5,\n                     labels = c(\"A great deal\", \"A lot\", \"A moderate amount\", \"A little\", \"None\")) +\n  labs(x = \"Concern about government undermining the news media\",\n       y = \"Trust in news media\") +\n  theme(panel.background = element_blank(),\n        axis.line = element_line())\n\n\n\n\nFigure 1: Scatterplot (with jitter) of perceived threat and distrust\n\n\n\n\nMost of that will look familiar from the previous plots we made. The only major difference is that instead of making a histogram I’m making a scatterplot, for which the “geometry” is points rather than histogram bars. Therefore I use geom_point() rather than geom_histogram() for the geometry layer. One new element is the position = position_jitter() part inside geom_point(). Its purpose is to add some random noise to each individual data point, moving it to the left or right a little bit along the x-axis and up or down a little along the y-axis. This is helpful here since there are many data points but only 5 possible answers along each axis. Try making the graph without including the jitter; it’ll just look like a grid of 25 points. Any pattern in the data will be impossible to see. It may seem counterintuitive to change the data by adding randomness, but for the purposes of the visualization, doing so actually makes any patterns easier to see."
  },
  {
    "objectID": "lab-4.html#interpreting-your-findings",
    "href": "lab-4.html#interpreting-your-findings",
    "title": "Lab 4: Analysis",
    "section": "Interpreting your findings",
    "text": "Interpreting your findings\nRemember, a correlation quantifies the general quantitative relationship between two sets of numbers; people’s answers to your perceived threat question are associated with their answer to the distrust question to the extent indicated by your correlation coefficient. To fully and fairly interpret this for your presentation and report you’ll need to consider a number of things:\n\nWhat features of how the questions were asked might have affected people’s answers?\nWhat, if anything, about the context in which the questions were asked might have influenced people’s answers?\nHow does it fit with the previous research you read about, and with your intuition about how the variables should be related?"
  },
  {
    "objectID": "lab-4.html#footnotes",
    "href": "lab-4.html#footnotes",
    "title": "Lab 4: Analysis",
    "section": "",
    "text": "Remember that, by convention, psychologists generally use \\(\\alpha = .05\\) as the criterion for statistical significance, meaning that if our data has less than a 5% chance of occurring under the null hypothesis we reject the null and tentatively accept the alternative hypothesis that the variables are associated.↩︎"
  },
  {
    "objectID": "lab-5.html#presentation",
    "href": "lab-5.html#presentation",
    "title": "Lab 5: Presentation & report",
    "section": "Presentation",
    "text": "Presentation\n\nGuide to presenting\nEach team will give a short presentation which should encapsulate the motivation, methods, anticipated findings, and interpretation of your proposed project. Aim for clarity, conciseness, and being bold to spark the audience’s interest in your topic and findings.\nAvoid simply reading excerpts from your report. That would be boring, and would probably take up too many words. Make it fun and interesting. Try to grab the audience’s attention and hit them with just the most important points of your ideas.\nMake your slides count. You can’t just cram a load of text on there, because nobody will be able to read it. Plus, it’d distract from what you’re saying. Make it a visual aid that somehow supports or clarifies what you’re saying. It might be a visual representation of your design, a key piece of your experimental stimuli, a graph of your expected results, or just a pertinent meme which conveys the motivation for your research question.\nAfter your presentation the group will take a few questions from the audience, and your responsiveness will contribute toward your grade as well as the quality of your presentation itself. Remember a perfectly acceptable answer is often: “Good question; I don’t know the answer! But here are some thoughts…”). It’s not usually an issue, but just in case your audience is left speechless, I suggest coming with a couple of questions or thoughts of your own that you can throw at the audience to spark more questions (“You might be wondering…”).\n\n\nGuide to watching presentations\nAs an audience member, you are still being graded for class participation. That means giving everyone else’s presentation the attention and enthusiasm it deserves, and rewarding their hard work with questions. (Going to the trouble of putting together a presentation only for nobody to have anything to say about it is not a good feeling.)\nGood questions to ask are things like “Could you clarify X”, “Had you considered Y”, or “How might this relate to Z.” One reason for presenting your project is to hopefully get some useful feedback from the audience with which to refine your final paper, so try to give the kind of feedback you hope to receive."
  },
  {
    "objectID": "lab-5.html#sec-report",
    "href": "lab-5.html#sec-report",
    "title": "Lab 5: Presentation & report",
    "section": "Written report",
    "text": "Written report\nYou will produce a miniature research paper reporting your project. Note that each team member will produce their own individual report; even though the project has been collaborative, your write up will be your own.\n\nFormat\nYour report should consist of the following sections:\n\nIntroduction (two or three paragraphs, including the general research question; summary of relevant research; and hypothesis)\nMethod (a description of your variables, the number of valid responses, and any other information about the procedures that generated the data that you think necessary to report)\nResults (a technical report of any descriptive statistics, figures, and statistics you produced)\nDiscussion (a paragraph or two interpreting your results and drawing conclusions)\n\n\n\nDeadline\nThe report is due by the next class (see late policy from Syllabus 3.2)."
  },
  {
    "objectID": "lab-5.html#grading",
    "href": "lab-5.html#grading",
    "title": "Lab 5: Presentation & report",
    "section": "Grading",
    "text": "Grading\nYou will receive scores out of 100 for your presentation and report. See Appendix D for general qualitative criteria which will be assessed in the context of the expectations detailed above."
  },
  {
    "objectID": "lab-6.html#project-overview",
    "href": "lab-6.html#project-overview",
    "title": "Lab 6: Project planning",
    "section": "Project overview",
    "text": "Project overview\n\nUnderstanding ANOVA\nANOVA stands for ANalysis Of VAriance. It is a statistical procedure which quantifies the relative contribution of difference sources of variability (variability between groups and variability within groups) to understand whether there are differences between groups over and above what would be expected by chance (i.e, based on the random variability within the groups).\nANOVA will always have a single, continuous dependent variable (DV). ANOVA can have any number of independent variables (IV; also called factors), each with any number of conditions (also called levels). The design will be described as something like “a 3x5 ANOVA”, which would mean that there were 3 factors with 5 levels each. For this project, your design will be a 2x2 ANOVA, meaning 2 factors, each with 2 levels.\n\nQuasi-independent variables\nResearchers often “manipulate” at least one of the IVs in a study, making it an “experiment.” In this context, the name “independent variable” implies that the researcher randomly determines which experimental group a participant will be in; therefore, the manipulated variable is theoretically “independent” of other variables in the study. The ANES, however, consists only of survey data; there is no experimental random assignment.1 Categorical variables that are not randomly assigned are called quasi-independent variables; they can still be used to look for differences between groups of participants, but since participants’ membership of a group is an existing characteristic of theirs rather than something randomly determined, it isn’t truly “independent” of other variables.\nIt may sound like a limitation, but there are good reasons to use quasi-IVs in research. It isn’t always possible or ethical to randomly assign group membership, yet there is still value in comparing existing groups. As an example, a participant’s age can’t be randomly assigned, but it is still often useful to compare different age groups to look for differences on some dependent variable. That said, there are important implications for causal inference that you should consider when it comes time to interpret your findings.\n\n\nInteraction\nThe value of a factorial ANOVA design is in revealing the “interaction” between variables: does the effect of one (quasi)-IV depend on another (quasi)-IV? For this project, we will stick with the idea of trust: it will be the DV. One of the quasi-IVs will be an indicator of political ideology/partisanship. It makes sense that some types of social trust could depend on your political preferences (or something related, like whether your preferred political party is currently “winning”). But political affiliation isn’t the only thing that could be associated with trust. Maybe one’s life experiences also play a role. Having had, or not had, some particular experience might be associated with expressing more or less trust. But crucially, maybe the two variables interact: maybe the relationship (if any) between partisanship and trust depends on what kind of experience a person has had (or not had). It could be that an experience affects trust among people who lean one way but not people who lean the other.\n\n\n\nStep 1. Examine the data and identify your variables\n\nDependent variable\nYou dependent variable should be continuous (measured on a numeric scale). For this project, we will stick with the idea of trust.\n\nHow often can you trust the federal government in Washington to do what is right? (V201233)\nHow many of the people running the government are corrupt? (V201236)\nGenerally speaking, how often can you trust other people? (V201237)\nIn general, how much trust and confidence do you have in the news media when it comes to reporting the news fully, accurately, and fairly? (V201377)\n\n\n\n(Quasi)independent variables\nThe first of your quasi-independent variables will be some indication of participants’ political preferences.2\n\nWhat political party are you registered with, if any? (V201018)\nWho did you vote for? (V202073)\nIf you had to choose, would you consider yourself a liberal or a conservative? (V201201)\n\n\n\n\n\n\n\n\nYour second quasi-independent variable will be one of the “life experiences” that the ANES survey asks about:\n\nDo you personally know someone who moved to the U.S. from another country, or not? (V202561)\nDo you currently owe money on student loans, or not? (V202562)\nHave you ever received food stamps or another form of public assistance, or not? (V202563)\nDo you have a pension or a retirement account, such as an IRA, 401k, or similar, or not? (V202564)\nDo you regularly choose products because they are made in America, or not? (V202565)\nHave you displayed an American flag on your house or in your yard in the past year, or not? (V202566)\nHave you gone hunting or fishing in the past year, or not? (V202567)\nHave you used public transportation in the past year, or not? (V202568)\nDuring the past 12 months, were you or any of your family members stopped or questioned by a police officer, or did this not happen in the past 12 months? (V202456)\nHave you ever been arrested, or has that never happened to you? (V202457)\nDuring the past 12 months, have you contacted or tried to contact a member of the U.S. Senate or U.S. House of Representatives, or have you not done this in the past 12 months? (V202030)\nDuring the past 12 months, have you joined in a protest march, rally, or demonstration, or have you not done this in the past 12 months? (V202025)\nDuring the past 12 months, have you ever gotten into a political argument with someone, or have you not done this in the past 12 months? (V202024)\nMany people say they have less time these days to do volunteer work. What about you, were you able to devote any time to volunteer work in the past 12 months or did you not do so? (V202033) \n\n\n\n\nStep 2. Read relevant research\nSince you choice is more open-ended, I can’t point you towards a particular paper like I did for the previous project. You’ll have to think about your constructs and see if you can find relevant research. In particular, what psychological construct do you think your “life experience” variable might reflect or relate to?\n\nWhat to look for\nA published, scholarly journal article detailing an empirical finding relevant to your variables of interest. This might be a paper reporting one or several individual studies that the researchers conducted, or it may be a review paper or meta-analysis.3\n\n\nWhere to look\n\nGoogle Scholar\nGoogle Scholar searches the full text of scholarly articles. It casts a wide net, searching across all disciplines, and including books and other materials in addition to journal articles, so will likely find many articles not very relevant to the topic as well as those that are relevant.\n\n\nAPA PsycINFO\nThe link above should take you to PsycINFO, a database for scholarly psychology research (you can also search for psycinfo in a CLIO quicksearch). PsycInfo gives you the ability to do more focused searching than Google Scholar.\n\nYou can add many keywords and combine them with the Boolean operators AND, OR, and NOT by selecting them from the dropdown boxes.\nYou can select where your keywords should appear, i.e. in the title, abstract, or full text of articles. Selecting Word in Major Subject Heading can help narrow down your search to articles that are actually on the topic you’re interested in (rather than just containing the keyword).\n\n\n\n\n\nStep 3. Articulate your design and hypothesis\nAs before, you should be able to state your:\n\nOperational definitions (that is, the specific questions that participants were asked, per the codebook)\nThe constructs that those operational definitions measure\nYour hypotheses\n\nIn the context of a 2x2 ANOVA design, you will have three different hypotheses. For each IV, you will hypothesize the existence (or not) of a main effect; that is, do you expect to see a difference between the different categories of that IV by itself (ignoring differences on the other IV). In addition, you will have a hypothesis about the “interaction” of the two IVs; that is, do you expect them to have a combined effect? In more technical terms, do you expect that the effect of one IV depends on the level of the other IV?"
  },
  {
    "objectID": "lab-6.html#footnotes",
    "href": "lab-6.html#footnotes",
    "title": "Lab 6: Project planning",
    "section": "",
    "text": "That’s not quite true. Some elements of the survey are randomized, such as the order of certain questions or answer options. And the ANES often tests different versions of questions before including them in the full survey; answers to different versions of questions can be compared to check whether seemingly minor differences to wording substantively affect people’s answers.↩︎\nThere’s a whole literature about ideology and partisanship and I don’t expect you to get into the weeds with it, but I expect you’ll have some thoughts about the various strengths/limitations of these few questions.↩︎\nA meta-analysis pools the findings of many individual studies by different researchers into a single analysis.↩︎"
  },
  {
    "objectID": "lab-7.html#data-preparation",
    "href": "lab-7.html#data-preparation",
    "title": "Lab 7: Data preparation & analysis",
    "section": "Data preparation",
    "text": "Data preparation\n\nSet up\nAs before, the first step is to prepare your R environment by loading required packages and loading the data. The will be exactly the same as with the previous project.\n\nlibrary(anesr)\nlibrary(tidyverse)\ndata(timeseries_2020)\n\n\n\nSelect variables\nFor this example, I’m going to use the “trust the media” question–the same one I used for the correlation example–as my dependent variable.1 For the partisanship quasi-IV, I’ll use the party registration question. And for the life experience, I’m going to use one I didn’t let you choose from (because not many people said yes, and… well, you’ll see): “Have you ever been bitten by a shark, or not?” (V202569).\n\nmy_data_raw &lt;- timeseries_2020 |&gt; \n  haven::zap_labels() |&gt; \n  select(trust = V201377, \n         party = V201018,\n         experience = V202569)\n\n\n\nRecode variables\nIn the raw data responses to the categorical quasi-IVs are coded as numbers: 1s and 2s for “Democrat”/“Republican” and “yes”/“no”, and some other numeric codes for other responses or invalid/missing answers. You’ll need to check the codebook to determine exactly what all the numbers mean. Our first step is to “recode” these raw numbers into more meaningful answers, keeping only the ones we need and turning other answers, or negative numbers representing missing data, into NA–R’s representation of missing data.\nWe’ll use dplyr’s mutate() function, which creates new variables or changes (mutates) existing ones. To achieve the desired result we’ll use dplyr’s case_when() function, which allows us to specify a condition and the value to assign if that condition is met. So for example party == 1 ~ \"Democratic\" can be read as “if the value of party is 1 then assign the value ‘Democratic’.” Since the original variable is named “party” and I’m using that name for the mutated variable, the column will be changed in place. Note that I only specify the numeric codes that I want to retain for my analysis; answers other than 1 or 2 will become NA since I don’t specify otherwise.\n\nmy_data &lt;- my_data_raw |&gt; \n  filter(if_all(everything(), ~ . &gt; 0)) |&gt;\n  mutate(party = case_when(\n    party == 1 ~ \"Democratic\",\n    party == 2 ~ \"Republican\"\n  )) |&gt; \n  mutate(experience = case_when(\n    experience == 1 ~ \"Yes\",\n    experience == 2 ~ \"No\"\n  ))"
  },
  {
    "objectID": "lab-7.html#analysis",
    "href": "lab-7.html#analysis",
    "title": "Lab 7: Data preparation & analysis",
    "section": "Analysis",
    "text": "Analysis\n\nDescriptive statistics\nBefore computing the ANOVA itself, the first step is to compute descriptive statistics–the mean and standard deviation–for each grouping of participants in our data. With a 2x2 design there are a few different ways to group up the data, corresponding to the 3 hypotheses we aim to test.\nFirst we’ll find the “marginal means”. These are relevant to testing and interpreting the “main effects”. Finding this kind of mean is slightly more complicated than when we just needed to find the mean of each column in a data.frame like we did for the correlation project. This time, we want the mean from one column broken into groups based on the value of a different column; mean trust by party affiliation, and mean trust by life experience.\nAs usual there are a lot of ways of doing this, but one of the most powerful is dplyr’s summarize() function. It allows you to compute summary values using other functions, such as mean(trust) to compute the mean of the trust column, and you can specify a grouping variable using the .by argument. And you aren’t limited to computing a single summary variable; here I also compute the standard deviation (using the sd() function) and number of observations in each group (using the n() function).\n\nmy_data |&gt; \n  summarize(mean = mean(trust), \n            sd = sd(trust),\n            n = n(),\n            .by = party) |&gt; \n  drop_na()\n\n# A tibble: 2 × 4\n  party       mean    sd     n\n  &lt;chr&gt;      &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt;\n1 Republican  1.75 0.980  1176\n2 Democratic  3.14 1.08   1666\n\nmy_data |&gt; \n  summarize(mean = mean(trust), \n            sd = sd(trust),\n            n = n(),\n            .by = experience)\n\n# A tibble: 2 × 4\n  experience  mean    sd     n\n  &lt;chr&gt;      &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt;\n1 No          2.53  1.22  3763\n2 Yes         2.32  1.04    22\n\n\nIn addition to those marginal means we need to find all 4 “cell means,” a mean trust score for each of the possible combinations of the political party and life experience groups: Democrats who have been bitten by a shark; Democrats who haven’t been bitten by a shark; Republicans who have been bitten by a shark; Republicans who haven’t been bitten by a shark. This sounds like a lot of work, but thankfully it can be achieved with a very minor tweak to the summarize() approach we used before: we just supply both grouping variables to the .by argument at the same time, “collecting” the variable names together with the c() function.\nThinking ahead, I’m going to want to make a graph showing these 4 means, and I’d like the graph to have “error bars” representing the confidence interval for each mean. There’s no built-in function to compute a confidence interval, so I’m going to make my own.\n\nci &lt;- function(x) {\n  qt(0.975, df = length(x) - 1) * sqrt( var(x) / length(x))\n}\n\nNow I’m ready to compute the mean, SD, number of observations, and confidence interval for each of the 4 groups.\n\nmy_data_summary &lt;- my_data |&gt; \n  summarise(mean = mean(trust), \n            sd = sd(trust),\n            n = n(),\n            ci = ci(trust),\n            .by = c(party, experience)) |&gt; \n  drop_na()\n\nmy_data_summary\n\n# A tibble: 4 × 6\n  party      experience  mean    sd     n     ci\n  &lt;chr&gt;      &lt;chr&gt;      &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt;  &lt;dbl&gt;\n1 Republican No          1.75 0.981  1170 0.0563\n2 Democratic No          3.14 1.08   1659 0.0522\n3 Republican Yes         1.5  0.837     6 0.878 \n4 Democratic Yes         2.71 0.488     7 0.451 \n\n\nNote that I assigned this data summary to a new name, my_data_summary, so it becomes a new data.frame object in my environment. That’s going to be useful later, because it is these summary statistics that I will use to make a graph of the results.\n\n\nANOVA\nThe aov() function computes an ANOVA. By itself, aov() doesn’t output all the information we want to see; that’s why I pipe it into the summary() function below.\nThe first argument to the aov() function is a formula, in the form DV ~ IV1 * IV2. The second argument is data, to which I supply the name of the data.frame containing my data; that’s how the formula in which we name the columns can work, since supplying the data.frame to the data argument tells the function where to find those columns.2\n\naov(trust ~ party * experience, data = my_data) |&gt; \n  summary()\n\n                   Df Sum Sq Mean Sq  F value Pr(&gt;F)    \nparty               1 1343.3  1343.3 1238.597 &lt;2e-16 ***\nexperience          1    1.5     1.5    1.421  0.233    \nparty:experience    1    0.1     0.1    0.098  0.754    \nResiduals        2838 3077.9     1.1                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n943 observations deleted due to missingness\n\n\nYou should see three lines with your variable names, followed by a line for “Residuals” (which you can ignore). Those three lines are your three hypothesis tests: two main effects and the interaction. Each line shows the “degrees of freedom” (“Df”), \\(F\\) value, and \\(p\\)-value (“Pr(&gt;F)”). (Sum Sq and Mean Sq don’t need to be reported). If the \\(p\\)-value is less than \\(0.05\\), that test is “statistically significant.”\nInterpreting this can be difficult, so we’ll follow this up in the next lab with some visualization which can make understanding the pattern of results easier."
  },
  {
    "objectID": "lab-7.html#footnotes",
    "href": "lab-7.html#footnotes",
    "title": "Lab 7: Data preparation & analysis",
    "section": "",
    "text": "In general, how much trust and confidence do you have in the news media when it comes to reporting the news fully, accurately, and fairly?↩︎\nIt would be more elegant if I could pipe the data into aov() and then pipe that into summary(), like this:\n  my_data |&gt; \n    aov(trust ~ party * experience) |&gt; \n    summary()\nUnfortunately that won’t work, because by default the pipe operator inserts the output of the previous line of code into the first argument of the next line. Since aov() is a base R function and the pipe is a more recent innovation, aov() wasn’t designed with this in mind. If it was, data would be the first argument and the pipe would work. There is a way to make it work, by using the _ special character, which R’s pipe operator understands as a placeholder for the pipe’s output, to specifically place the output into aov()’s data argument:\n  my_data |&gt; \n    aov(trust ~ party * experience, data = _) |&gt; \n    summary()\nIt’s not very elegant in this case, which is why I didn’t bother, but knowing about the _ placeholder can be useful in general.↩︎"
  },
  {
    "objectID": "lab-8.html#visualizing-anova",
    "href": "lab-8.html#visualizing-anova",
    "title": "Lab 8: Visualization & interpretation",
    "section": "Visualizing ANOVA",
    "text": "Visualizing ANOVA\nIn the example code for the previous lab session, I created a summary of descriptive statistics named my_data_summary. It included the average trust score for each of the 4 combinations of my 2 IVs, as well as a 95% confidence interval. I’m going to use that summary data.frame (rather than the full data) to create a graph of the interaction.\nThere are two main ways of visualizing the interaction: as a bar graph, or as a line graph. They show exactly the same information just in a slightly different way, and which you find to be better–i.e., more intuitive and effective for conveying your findings–is a matter of personal preference.\n\nBar graph\n\nmy_data_summary |&gt; \n  ggplot(aes(x = party, y = mean, fill = experience)) +\n  geom_col(position = position_dodge()) +\n  geom_errorbar(aes(ymax = mean + ci, ymin = mean - ci), position = position_dodge(width = 0.9), width = 0.3) +\n  coord_cartesian(ylim = c(1, 5))\n\n\n\n\n\n\nLine graph\nHere I use several aesthetic mappings for the “life experience” variable: the color of the lines and points, the shape of the points and linetype of line (solid or dashed) all differ. It’s redundant to map the same variable to so many aesthetics, but the redundancy can be helpful. If the graph was printed in black and white, for example, it would still be possible to easily tell the groups apart.\n\nmy_data_summary |&gt; \n  ggplot(aes(x = party, y = mean, group = experience, color = experience, linetype = experience, shape = experience)) +\n  geom_point(size = 3) +\n  geom_line() +\n  geom_errorbar(aes(ymax = mean + ci, ymin = mean - ci), width = 0.1) +\n  coord_cartesian(ylim = c(1, 5))\n\n\n\n\nMy graphs here are very rudimentary; remember, you should customize aspects of the theme and labels to better convey the data and conform to the usual APA style for figures."
  },
  {
    "objectID": "lab-8.html#interpreting-anova",
    "href": "lab-8.html#interpreting-anova",
    "title": "Lab 8: Visualization & interpretation",
    "section": "Interpreting ANOVA",
    "text": "Interpreting ANOVA\nAs a reminder, a 2x2 ANOVA tests 3 different hypotheses: 2 main effects and the interaction. The outcomes of all 3 are independent: you may find that both main effects are significant, or neither, or one main effect is significant but not the other. And the interaction may be significant (or not) regardless of the (non)significance of the main effects.\nThis can get quite complicated to think about, so try to break it down in the context of your design. Think about:\n\nThe main effect of partisanship (do Democrats and Republicans differ in trust?)\nMain effect of life experience (do people who have and have not had the experience in question differ in trust?)\nInteraction: does the relationship between the life experience and trust depend on partisanship? I.e., is the relationship different for Democrats vs Republicans?\n\nYour report should mention all three of the above issues."
  },
  {
    "objectID": "lab-9.html#presentation",
    "href": "lab-9.html#presentation",
    "title": "Lab 9: Presentation & report",
    "section": "Presentation",
    "text": "Presentation\nThe guide to delivering an effective presentation is the same as last time. My main advice this time is to focus on making your interpretation of the main effects and interaction as clear and convincing as possible. As you’ve probably seen, it’s tricky to get your head around, and even trickier to explain to someone else. The best presentations will make the logic of your interaction as easy to follow as possible."
  },
  {
    "objectID": "lab-9.html#sec-report",
    "href": "lab-9.html#sec-report",
    "title": "Lab 9: Presentation & report",
    "section": "Written report",
    "text": "Written report\nYou will again produce a miniature research paper reporting your project. Note that each team member will produce their own individual report; even though the project has been collaborative, your write up will be your own.\n\nFormat\nYour report should consist of the following sections:\n\nIntroduction (two or three paragraphs, including the general research question; summary of relevant research; and hypothesis)\nMethod (a description of your variables, the number of valid responses, and any other information about the procedures that generated the data that you think necessary to report)\nResults (a technical report of any descriptive statistics, figures, and statistics you produced)\nDiscussion (a paragraph or two interpreting your results and drawing conclusions)\n\n\n\nDeadline\nThe report is due by the next class (see late policy from Syllabus 3.2)."
  },
  {
    "objectID": "lab-9.html#grading",
    "href": "lab-9.html#grading",
    "title": "Lab 9: Presentation & report",
    "section": "Grading",
    "text": "Grading\nAs before you will receive a scores out of 100 for your presentation and report. See Appendix D for general qualitative criteria which will be assessed in the context of the expectations detailed above."
  },
  {
    "objectID": "lab-10.html#project-overview",
    "href": "lab-10.html#project-overview",
    "title": "Lab 10: Project planning",
    "section": "Project overview",
    "text": "Project overview\nFor this project you have free choice of design and constructs. You may look at the correlation(s) between two (or more) variables, or you may use an ANOVA design to look at differences between groups based on categorical variables. The process for getting started planning your design is the same as before.\n\nStep 1. Decide on your constructs/operational definitions\nThe full ANES 2020 dataset contains more than 1,000 columns, each representing a different variable that was measured. To navigate this, see the codebook, or the Methodology Report which organizes things into “modules”. Not all of these are equally interesting from a social-psychological perspective, but you will find many questions that pertain to fundamental social-psychological concepts. Here are some suggestions–constructs the ANES data measures in some way that you might consider from a social psychological perspective:\n\nFeeling thermometers (affect towards various political actors/groups)\nPolitical knowledge\nMedia consumption\nAuthoritarianism\nStereotypes\nPolicy preferences\nConspiracy beliefs\nReligiosity\nLife experiences\n\n\n\nStep 2. Find relevant research\nYou should search the literature (using Google Scholar or PsycINFO, as before) and find at least one published research paper that pertains to the constructs you want to look at.\n\n\nStep 3. Articulate your hypotheses\nAs before, you should be able to describe:\n\nThe social psychological constructs you are interested in\nThe operational definitions used to measure those constructs in the ANES survey\nYou hypothesis about how your constructs relate"
  },
  {
    "objectID": "lab-11.html",
    "href": "lab-11.html",
    "title": "Lab 11 & 12: Analysis, visualization & interpretation",
    "section": "",
    "text": "Groups should have decided on what variables to use and what analysis is appropriate. By the end of these sessions you should have coded your analysis, interpreted the results, and be ready to present and write up your findings.\n\nData wrangling tips\nSince everyone will be doing different things, Professor Brotherton work with groups individually to provide guidance on data preparation and analysis where help is needed. However, here are some hints about how to achieve common tasks.\n\nCompute an average of several variables.\nSome psychological measures consist of more than one question, and you need to compute the average of each participant’s answers to all the relevant questions. mutate() and rowMeans() can be used.\n\nmy_data |&gt; \n  mutate(mean = rowMeans(across(all_of(scale_vars))))\n\n\n\nCompute a sum score.\nIf you need to add scores across several questions, rowSums() can be used.\n\nmy_data |&gt; \n  mutate(sum = rowSums(across(all_of(scale_vars))))\n\n\n\nNtiles.\nOccasionally you might like to split a continuous measure into “ntiles,” meaning a number of roughly equally-sized groups. The ntile() function can be used to this.\n\nmy_data |&gt; \n  mutate(income_bracket = ntile(income, 2))\n\n\n\nArbitrary groups\n\nmy_data |&gt; \n  mutate(age_group = case_when(age &lt; 30 ~ \"Young\", age &gt;= 30 ~ \"Old\"))"
  },
  {
    "objectID": "lab-12.html#presentation",
    "href": "lab-12.html#presentation",
    "title": "Lab 13: Presentation & report",
    "section": "Presentation",
    "text": "Presentation\nSee again the general tips for delivering an effective presentation. My main advice this time is to remember that every group will likely be working on quite different social psychological topics; your audience will not be experts in the topic you have chosen. Focus on making sure you give a clear overview of your constructs, operational definitions, and hypotheses, all in the context of whatever relevant previous research you have discovered."
  },
  {
    "objectID": "lab-12.html#sec-report",
    "href": "lab-12.html#sec-report",
    "title": "Lab 13: Presentation & report",
    "section": "Written report",
    "text": "Written report\nYou will again produce a miniature research paper reporting your project. Note that each team member will produce their own individual report; even though the project has been collaborative, your write up will be your own.\n\nFormat\nYour report should consist of the following sections:\n\nIntroduction (two or three paragraphs, including the general research question; summary of relevant research; and hypothesis)\nMethod (a description of your variables, the number of valid responses, and any other information about the procedures that generated the data that you think necessary to report)\nResults (a technical report of any descriptive statistics, figures, and statistics you produced)\nDiscussion (a paragraph or two interpreting your results and drawing conclusions)\n\n\n\nDeadline\nThe report is due by the next class (see late policy from Syllabus 3.2)."
  },
  {
    "objectID": "lab-12.html#grading",
    "href": "lab-12.html#grading",
    "title": "Lab 13: Presentation & report",
    "section": "Grading",
    "text": "Grading\nAs before you will receive a scores out of 100 for your presentation and report. See Appendix D for general qualitative criteria which will be assessed in the context of the expectations detailed above."
  },
  {
    "objectID": "appendix-r-basics.html#posit.cloud",
    "href": "appendix-r-basics.html#posit.cloud",
    "title": "Appendix A — Getting started with R",
    "section": "posit.cloud",
    "text": "posit.cloud\nYou will use posit.cloud to write R code and work with data in RStudio. To use it you’ll just need to sign up for a free account.\n\nLet’s do something cool\nOnce you have a posit.cloud account, click this link.\nOnce the project is up and running, click on anes.R in the bottom-right pane to open some analysis code.\n\n\nWait, what are you talking about?\nThere are a few different names involved here, so to try and clear things up:\n\nR is a coding language\nRStudio is a software interface for using R\nPosit is the name of the company that makes RStudio\nposit.cloud provides a way of using RStudio in your web browser\n\nYou can install R and RStudio on your own computer for free and do things that way, but using the cloud-based RStudio via posit.cloud simplifies things immensely."
  },
  {
    "objectID": "appendix-r-basics.html#fundamentals-of-r-for-data-analysis",
    "href": "appendix-r-basics.html#fundamentals-of-r-for-data-analysis",
    "title": "Appendix A — Getting started with R",
    "section": "Fundamentals of R for data analysis",
    "text": "Fundamentals of R for data analysis\nR is a programming language well-suited to interactive data exploration and analysis. It might seem daunting if you’ve have no experience with coding, but the basic idea is that you have some data, like you are familiar with from a regular Excel or Google Sheets spreadsheet, and you perform operations on your data using functions a lot like you would in Excel/Sheets. For example, you might compute an average in Sheets by typing =AVERAGE(A1:A10). In R you might type mean(my_data$column_a). The specifics of the function names are different, but the basic idea is the same.\nHere are some of the basics to help you get started coding in R.\n\nRStudio\nRStudio is the interface we’ll use to write and run R code and see its output. The interface has 4 panels, each with a few tabs:\n\n\nTop-left: Code editor / data viewer\n\nYou will type code here\nYou can run a line of code by clicking on it and pressing Ctrl/Cmd + Enter on your keyboard\n\nBottom-left: R console\n\nYou can type code directly and run it by pressing enter.\nYou won’t be saving your code as a document like when you type in in the editor, so this is useful for just testing something before you commit it to your working document\n\nTop-right: Environment\n\nAs you excute code you may be creating objects like sets of numbers of data.frames. Those objects will appear here.\nYou can click the name of some objects, like data.frames, and it will open a view of the data as a tab in the editor pane\n\nBottom-right: Files/folders, plot viewer, help window\n\nYou can navigate the file tree, and you will see any plots you create appear here\n\n\n\n\nAssignment\nR has a fancy assignment operator: &lt;-.1 You assign things to a name by typing something like:\n\nname &lt;- thing\n\nThe thing there might be a set of numbers, an entire dataset, or something else. Giving it a name allows to you perform subsequent operations more easily, and choosing appropriate names makes your code easier to understand.\n\noriginal_numbers &lt;- 1:10\noriginal_numbers\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\ndoubled_numbers &lt;- original_numbers * 2\ndoubled_numbers\n\n [1]  2  4  6  8 10 12 14 16 18 20\n\n\n\n\nFunctions\nAlmost everything happens inside functions.\n\nmean(original_numbers)\n\n[1] 5.5\n\nmean(doubled_numbers)\n\n[1] 11\n\n\nYou can also nest functions inside one another.\n\nsqrt(mean(original_numbers))\n\n[1] 2.345208\n\n\nA function generally has one or more “arguments”, to which you supply parameters. For example, the mean() function’s first argument is the set of numbers you want to compute the mean of; in the previous examples original_numbers and doubled_numbers were the parameters I supplied. You don’t necessarily have to type the name of the argument, but it can be helpful. The seq() function, for example, produces a sequence of numbers according to three arguments, from, to, and by.\n\nseq(from = 1, to = 10, by = 2)\n\n[1] 1 3 5 7 9\n\n\nWhen you don’t type the names of the arguments, R matches them by position, so this gives exactly the same output as the previous line of code:\n\nseq(1, 10, 2)\n\n[1] 1 3 5 7 9\n\n\nYou can get help with a function (to see what arguments it accepts, for example) by typing a question mark followed by the function name (without parentheses) in your console.\n\n?mean\n\nRunning the code will bring up the function’s help documentation in RStudio’s Help pane.\n\n\nPiping\nYou can string together different operations in a pipeline using the pipe operator: |&gt;.2 The result of each line of code gets “piped” into the function on the next line as its first argument. For example, below I take some data (named data) and perform a series of operations, first selecting a subset of columns, then filtering rows based on whether the values in certain columns meet specified criteria, then I create (mutate) a new column averaging across existing columns; and lastly, I summarize the new column down to an average value.\n\ndata |&gt; \n  select(column_a, column_b) |&gt; \n  filter(if_all(c(column_a, column_b), ~!is.na(.))) |&gt; \n  mutate(column_c = rowSums(across(everything()))) |&gt; \n  summarize(mean_sum = mean(column_c))\n\nThere’s a lot going on there, and the specifics will become clearer as we work though this project. But using the pipe operator this way can make for relatively readable code."
  },
  {
    "objectID": "appendix-r-basics.html#footnotes",
    "href": "appendix-r-basics.html#footnotes",
    "title": "Appendix A — Getting started with R",
    "section": "",
    "text": "Most other coding languages tend to use a boring = for assignment. Sure it’s nice not having to type an extra character, but there’s a keyboard shortcut to quickly add an &lt;- in RStudio: Option/Alt + -. And philosophically, the &lt;- arrow conveys the inherent directionality of the assignment operation. The object is assigned to the name; the object and its name are not equal and so the = arguably gives a misleading impression of the two things being one and the same. (Also, to let you in on a secret, = also works for assignment in R.)↩︎\nIf you’re looking at R code from beyond this handbook (e.g. looking up help elsewhere) you may see a different pipe: %&gt;%. The |&gt; pipe, called the “native” pipe, was only included as a feature of base R relatively recently. Until then, the %&gt;% pipe was provided by an external package (called magrittr. Get it?). In practice the pipes work similarly, so you can often just replace %&gt;% with |&gt; and it’ll work fine, but it’s worth being aware of.↩︎"
  },
  {
    "objectID": "appendix-describing-data.html#the-mean",
    "href": "appendix-describing-data.html#the-mean",
    "title": "Appendix B — Describing data",
    "section": "The mean",
    "text": "The mean\nThe average, or mean, is a measure of typical performance; it summarizes all the scores and produces a single number which represents the most typical value. The basic formula for the mean of a set of scores is:\n\\[\nM = \\dfrac{\\Sigma X}{n}\n\\]\nIn this equation, \\(X\\) refers to all the scores in the group, and \\(n\\) is the number of scores in the group. The symbol \\(\\Sigma\\) instructs you to sum all the scores. A simple way of saying the formula in words is: Add up all the scores in the group and divide by the number of scores in that group."
  },
  {
    "objectID": "appendix-describing-data.html#standard-deviation",
    "href": "appendix-describing-data.html#standard-deviation",
    "title": "Appendix B — Describing data",
    "section": "Standard deviation",
    "text": "Standard deviation\nHowever, there is always variability in the scores in a group. The mean is a central value, but some scores fall below it and others above it. Therefore, researchers also need to describe the amount of variability in scores. This puts the mean in context, describing just how representative of all the scores it is. If there is high variability, scores are spread widely and the mean is relatively unrepresentative; if there is low variability, scores are clustered tightly and the mean is relatively representative.\nA mathematical way of describing the amount of variability in a group of scores is to calculate the deviation of each score from the mean, square the deviations, and then sum the squared deviations. This quantity is called Sum of Squares (SS). One mathematical formula is:\n\\[\nSS = \\Sigma(X - M)^2\n\\]\nDividing \\(SS\\) by the number of scores in the group minus 1 produces a quantity called variance, which is represented by the symbol \\(s^2\\). Variance is the average squared deviation. (Remember that to calculate an average, you add a set of scores and divide by \\(n\\). Here we add a set of deviations and divide by \\(n – 1\\). We use \\(n – 1\\), rather than just \\(n\\), because it is a necessary statistical adjustment to account for the fact that samples tend to underestimate variability.)\n\\[\ns^2 = \\dfrac{\\Sigma(X-M)^2}{n-1}\n\\]\nTaking the square root of the variance produces another quantity, called standard deviation. It is represented mathematically by the symbol \\(s\\), but in psychology papers you will most often see it represented by the letters SD.\n\\[\nSD = \\sqrt{\\dfrac{\\Sigma(X-M)^2}{n-1}}\n\\]\nWhile variance is the average squared deviation, SD is the average deviation in the original units (i.e. not squared). This is the most intuitive way to convey how much scores typically varied about the mean."
  },
  {
    "objectID": "appendix-correlation.html#calculating-the-correlation-coefficient",
    "href": "appendix-correlation.html#calculating-the-correlation-coefficient",
    "title": "Appendix C — Correlation",
    "section": "Calculating the correlation coefficient",
    "text": "Calculating the correlation coefficient\nTo calculate the correlation between two variables, you must first calculate the Sum Product, \\(SP\\). The mathematical formula is:\n\\[\nSP = (X-M_X)(Y-M_Y)\n\\]\nNotice that \\(X - M_X\\) and \\(Y - M_Y\\) are deviation scores, just like we calculated for the standard deviation. Here we have two variables, \\(X\\) and \\(Y\\), so the equation is telling us to calculate the deviation of each score from its respective mean. We then multiply each deviation for variable \\(X\\) by its counterpart deviation from variable \\(Y\\). These are the “products,” meaning multiplied deviation scores. Finally, the tells us to add up all those products, giving the “sum of products,” \\(SP\\).\nOnce we have calculated \\(SP\\), the correlation coefficient, symbolized by \\(r\\) is calculated using the following equation:\n\\[\nr = \\dfrac{SP}{\\sqrt{SS_X SS_Y}}\n\\]\nHere, \\(SS_X\\) and \\(SS_Y\\) are the Sums of Squares for each variable. Multiplying them and taking the square root gets us a measure of the variability in \\(X\\) and \\(Y\\) separately. The numerator, \\(SP\\), represents the covariability of \\(X\\) and \\(Y\\). So the equation results in covariability as a proportion of all variability. It can range from \\(-1\\), meaning a perfect negative correlation, to \\(0\\), meaning no correlation at all, to \\(+1\\), meaning a perfect positive correlation."
  },
  {
    "objectID": "appendix-correlation.html#sec-cor-effect-size",
    "href": "appendix-correlation.html#sec-cor-effect-size",
    "title": "Appendix C — Correlation",
    "section": "Effect size for correlation",
    "text": "Effect size for correlation\nThe correlation coefficient is a measure of effect size. It’s absolute value can range from 0 to 1.\nYou may see some “rules of thumb” about interpreting the “effect size” of correlations in psychology. Cohen (1977) proposed that correlations of less than around \\(\\pm 0.30\\) should be considered weak; around \\(\\pm 0.30\\) to \\(\\pm 0.70\\) considered moderate; and greater than around \\(\\pm 0.70\\) considered large.\nHowever, more recent researchers have proposed more nuanced and empirically-grounded interpretations. Funder and Ozer (2019) proposed the following:\n\n\n\n\n\n\n\n\n\nr\nDescription\n\n\n\n\n0.05\nVery small for the explanation of single events but potentially consequential in longer run\n\n\n0.10\nStill small at the level of single events but potentially more ultimately consequential\n\n\n0.20\nMedium effect of some explanatory and practical use even in the short run\n\n\n0.30\nLarge effect that is potentially powerful in both the short and the long run\n\n\n0.40\nA very large effect in the context of psychological research; likely to be a gross overestimate"
  },
  {
    "objectID": "appendix-rubric.html",
    "href": "appendix-rubric.html",
    "title": "Appendix D — Grading rubric",
    "section": "",
    "text": "For each presentation and project report you will receive a score out of 100 according to the rubric below.\nNote that presentations will be given jointly. In general, grades will be the same for all group members as it is expected that group members will contribute equally to the presentation; however, exceptions may be made when it is clear that group members did not all contribute equally.\nAlso note that even though projects will be a group effort, the written reports will be completed individually.\n\n\n\n\n\n\n\n\nGrade\nPoint range\nDescription\n\n\n\n\nA+\n97-100\nOutstanding and exceptional work. Clearly articulates the problem, purpose, methods, results, and interpretation. There is evidence of critical thought, and the work goes beyond the assignment requirements in terms of analysis or presentation, demonstrating a sophisticated understanding of the concepts and techniques used. The presentation/report is free of errors and is clearly and professionally executed\n\n\nA\n90-97\nExcellent work. Clearly articulates the problem, purpose, methods, and results. There is evidence of critical thought, demonstrating a strong understanding of the concepts and techniques used. The presentation/report is virtually free of errors and is clearly and professionally executed\n\n\nB\n80-89\nAbove average work. Articulates the problem, purpose, methods, and results. There is some evidence of critical thought. Demonstrates a good understanding of the concepts and techniques used. There are minor errors or lack of clarity in some aspects, but the presentation/report is generally clear and professional.\n\n\nC\n70-79\nSatisfactory work. Articulates the problem, purpose, methods, and results but may lack clarity or detail. There is minimal evidence of critical thought. Demonstrates an acceptable understanding of the concepts and techniques used. There are noticeable errors, and the presentation/writing could be improved.\n\n\nD\n60-69\nBelow average work. Does not clearly articulate the problem, purpose, methods, results, or interpretation. There is little to no evidence of critical thought. Demonstrates a minimal understanding of the concepts and techniques used. There are significant errors, and the presentation/writing is unclear.\n\n\nF\n&lt;60\nUnsatisfactory work. Does not articulate the problem, purpose, methods, results, or interpretation. There is no evidence of critical thought. Demonstrates a lack of understanding of the concepts and techniques used. There are many errors, and the presentation/writing is poor."
  }
]