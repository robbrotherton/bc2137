# Lab 3: Data preparation {.unnumbered}

You will begin this session with variables from the ANES dataset in mind for your analysis. In class we will introduce the R language and RStudio environment, and demonstrate necessary data cleaning and manipulation in preparation for analysis. By the end of the class you should have modified the example code to work with your selected variables.

### Goals

-   Get your R environment set up
-   Read the data you need into R
-   Select required variables
-   Filter the data based on completeness (and any other criteria)
-   Compute any required variables (scale means, number of items missing, etc)
-   Describe and visualize your variables

## Working with data in R

### Getting R ready

To start exploring the data in R, you first need to set up your environment. While "base" R (meaning all the functions built in to the R language) can do everything we need, a feature of R that makes it well-suited for data analysis is that you can easily install additional "packages" that make common tasks, like data cleaning and visualization, even easier. In particular, we will use a family of related packages called the "tidyverse". Usually this would need to be installed like so:

```{r}
#| eval: false

install.packages("tidyverse")

```

However the Projects template I set up for you in posit.cloud already has the package installed. Packages only need to be installed once, so there's no need for you to do so again. You will, however, need to 'activate' the packages using the `library()` function to make their functions available.

```{r}
#| warning: false
#| message: false

library(tidyverse)

```


### Getting data into R

Getting data into R often involves reading in a .csv (comma-separated values) spreadsheet file that you downloaded to your computer. Indeed, you can download the ANES data file as a .csv from the ANES website. For our convenience, I have already done that. Your posit.cloud project has a folder called `data` and inside that folder there is a file named `anes_2024.csv`. That's the complete ANES data for the year 2024. To start working with that data, we first need to 'read' it into R.

```{r}

data <- read_csv("data/anes_2024.csv")

```

When you execute the code you won't see any output, but you should see the name `data` (or whatever name you assigned the result of `read_csv()` to) appear in your Environment pane. That is now an object in R called a data.frame. You can think of it as a spreadsheet like you're familiar with from Excel or Google Sheets; a set of columns, one for each variable in the dataset, and a row for each participant's answers.

In fact, you can also click on the name in the Environment pane to view the data in a new tab, just like looking at a spreadsheet. Or you can see a preview of the full spreadsheet using code:

```{r}

head(data)

```


### Select your variables

As you can see, the data.frame contains a *lot* of variables; there are `r scales::label_comma()(ncol(data))` columns of data, one for each recorded variable. You'll only need two of those. So the first step is selecting just the variables you need to work with.

There are a lot of ways to do this. The simplest would be to make a note of the variable IDs from the codebook and use the `select()` function.[^tidyverse-note] This allows us to simply type in variable names separated by commas.

For this example code I'll look at two variables similar to those I had you pick from: trust in news media (In general, how much trust and confidence do you have in the news media when it comes to reporting the news fully, accurately, and fairly?) and perceived threat to the media (How concerned are you that some people in the government today might want to undermine the news media’s ability to serve as a check on governmental power?). Their variable IDs are V241335 and V241334 respectively. Since I'll probably forget which ID is which, I'll give the columns more meaningful names as I select them.

```{r}

my_data <- data |> 
  select(trust = V241335, 
         threat = V241334) 

```

Let's see what this new data.frame looks like:

```{r}
head(my_data)
```

It all looks good so far. But if you inspect the data more extensively (click the name in your Environment to open a tab showing the data and scroll down a bit) you'll notice that there are some negative numbers in the data. You can see all the unique values recorded for a column in the data like so: 

```{r}
unique(my_data$trust)
```

The negatives are from survey codes which record missing data. If you try to calculate an average score with those included it'll mess up the sums, so we need to do some data cleaning to handle things like that.



### Cleaning the data

There are a lot of different ways we could handle this. One way is to `filter()` the data, retaining only rows which meet certain conditions.[^1]

The ANES coding scheme uses negative values for the various kinds of missing or inappropriate data, which makes things simple: only positive values are valid and should be retained.

To implement this as a `filter()`, we can use the `if_all()` function; i.e., we are going to select some columns and *if all* the values in those columns meet some condition the row will be retained. To select the columns we can use the `everything()` function, since the positive-valid/negative-invalid rule is true of every column in our data. The part after the comma, `~ . >= 0`, articulates the condition. The `~` prefix is necessary because instead of naming one specific column to refer to its values we use `.` as a placeholder representing the values in each of the selected columns; the value must be greater than or equal to 0 to be retained.[^2] 

```{r}
my_data_complete <- my_data |> 
  filter(trust >= 0, threat >= 0)
```

Notice that the number of rows in the data.frame has changed, because rows that didn't meet that condition have been dropped.

```{r}
nrow(my_data)
nrow(my_data_complete) 
```

After filtering to keep only rows with complete data, we're left with `r format(nrow(my_data_complete), big.mark = ",")` valid responses.


### Recoding values

Now that we have selected our columns and filtered out missing/invalid responses, the last thing to do is recode values so they all mean what we want them to mean.

Notice that valid responses for the "undermine the news media" threat item are 1 (not at all concerned) through 5 (extremely concerned). I want higher scores on that question to indicate greater perceived threat, so that's fine. 

For the "trust in news media" question, responses are 1 (none) though 5 (a great deal). But I'm thinking of the psychological construct as *dis*trust rather than trust, so I want higher scores to indicate more distrust. The solution is simple: subtract the participant's answer from 6 (one more than the maximum score) so that a response of 1 becomes a 5 (the most distrust), 2 becomes 4, 3 stays 3, 4 becomes 2 and 5 becomes 1 (the least distrust).


```{r}

my_data_complete <- my_data_complete |>
  mutate(distrust = 6 - trust)

```


Now I have my two variables, perceived threat to the news media, and (reverse-coded) distrust of the news media, for each of the `r scales::label_comma()(nrow(my_data_complete))` participants with complete data. We're ready to start exploring the data.


## Start examining the data

### Descriptive statistics

The most common descriptive statistics are the mean ($M$) and standard deviation ($SD$). You should report these for each variable in your analysis.

There are many ways of doing this, but for now I'll just use the `mean()` and `sd()` functions. I can refer to a particular column in a data.frame using the `$` operator, i.e. `my_data$threat` and so on.

```{r}
mean(my_data_complete$threat)
sd(my_data_complete$threat)

mean(my_data_complete$distrust)
sd(my_data_complete$distrust)
```


### Visualizing distributions

In addition to reporting the mean and standard deviation, it is useful to visualize the distribution of the data. This can reveal nuances that are not obvious in those single numeric summary values.

As with most things, there are a lot of different ways of producing graphs using R. One of the most widely used and powerful is the `ggplot2` package.^[The `ggplot2` package is part of the `tidyverse`, so because we already ran `library(tidyverse)` earlier the `ggplot2` functions are already available to us. If you needed to, you could always run `library(ggplot2)` to activate it separately.] The name refers to the idea of the "grammar of graphics", and it is built around a layering approach. You first specify your data and aesthetics (what should data will go on the X and Y axes), then geometry (do you want data to be represented by points or bars or as a histogram?), any scaling (e.g. what values should be labeled on each axis), and theme elements (how do you want the plot to look generally?). There can be a lot of complexity, but building things up layer by layer, gradually adding and refining elements, is a powerful and satisfying approach.

Here's a simple histogram of the threat item. I pipe the data into the `ggplot()` function, specifying that I want the `threat` column to be represented as the `x` aesthetic. Then I add geometry using `geom_histogram`. That geom function automatically computes bins and counts; here I just specify I want a `binwidth` of 1, i.e. each column of the histogram will represent one scale point. Note that ggplot layers are added using `+` rather than the usual `|>` pipe.


```{r}
#| label: fig-threat-no-theme
#| fig-cap: Histogram of responses to perceived threat to news media question

my_data_complete |> 
  ggplot(aes(x = threat)) +
  geom_histogram(binwidth = 1)

```


The default theme is perfectly serviceable, but you can customize every element. Here I'll specify a couple of aspects using the `theme()` function, and I'll assign it to the name `theme_apa`. Then I can always add `theme_apa` as a layer to my plots going forward. 

```{r}

theme_apa <- theme(
  panel.background = element_blank(),
  axis.line = element_line()
)

```

I'll also customize the "breaks" and "labels" on the x-axis (to show the verbal scale responses rather than just numeric codes), and the labels for the x and y axes.

```{r}
#| label: fig-threat
#| fig-cap: Histogram of responses to perceived threat to news media question

my_data_complete |> 
  ggplot(aes(x = threat)) +
  geom_histogram(binwidth = 1, color = "white") +
  scale_x_continuous(breaks = 1:5, 
                     labels = c(" Not at all", "A little", "Moderately", "Very", "Extremely")) +
  labs(x = "How concerned are you that some people in the government today might want to\nundermine the news media’s ability to serve as a check on governmental power?",
       y = "Number of responses") +
  theme_apa

```

Here's a histogram of the distrust item.

```{r}
#| label: fig-distrust
#| fig-cap: Histogram of responses to "distrust" question

my_data_complete |> 
  ggplot(aes(x = distrust)) +
  geom_histogram(binwidth = 1, color = "white") +
  scale_x_continuous(breaks = 1:5,
                     labels = c("A great deal", "A lot", "A moderate amount", "A little", "None")) +
  labs(x = "In general, how much trust and confidence do you have in the news media \n when it comes to reporting the news fully, accurately, and fairly?",
       y = "Number of responses") +
  theme_apa

```


## To do for next time

Read another paper. The more detailed your understanding of the existing research, the better you will be able to interpret the results of your analysis.

::: {.references}
Lalot, F., Abrams, D., & Travaglino, G. A. (2021). Aversion amplification in the emerging COVID‐19 pandemic: The impact of political trust and subjective uncertainty on perceived threat. *Journal of Community & Applied Social Psychology, 31(2),* 213-222. [https://doi.org/10.1002/casp.2490](https://onlinelibrary.wiley.com/doi/pdfdirect/10.1002/casp.2490)
:::



[^installing-packages-note]: If I hadn't already installed the packages for you, it generally wouldn't be too much trouble. Most packages, like `tidyverse`, can be installed by running a single like of code:

        install.packages("tidyverse")

    The `anesr` package is slightly more complicated since it exists only on github.com, not R's official repository of packages. Therefore installing it requires two lines of code:

        install.packages("devtools")
        devtools::install_github("jamesmartherus/anesr")
        
    The `devtools` packages provides the `install_github()` function that allows the `anesr` package to be installed from github!

[^tidyverse-note]: The `select()` function, along with `filter()`, `mutate()`, `across()`, `everything()`, and others that you'll see in my example code, is part of the `tidyverse` family of packages (specifically these all come from the `dplyr` package, but we'll also use functions from other `tidyverse` packages like `tidyr` and `ggplot2`). There are other ways to do all these things without using `tidyverse` packages, just relying on what's referred to as "base" R functions. The `tidyverse` approach just makes this kind of data manipulation generally easier and makes the code more interpretable. If you're curious to see how base R and tidyverse functions differ in syntax, a good place to start is <https://dplyr.tidyverse.org/articles/base.html>.

[^1]: Another way would be to `mutate()` the data, changing the invalid response codes into the value `NA`, R's special value to indicate missing data. This could be achieved like so:

        my_data_complete <- my_data |>
          mutate(across(everything(), ~replace(., . < 0, NA)))

    That would mutate (i.e. change values) across every column. You can read the second part (after the `~`) as "replace the original values (indicated by the placeholder `.`), where the value is less than zero, with `NA`.

[^2]: If the data wasn't as simple or if we just wanted to be more explicit about things, we could filter based on valid responses for each item.

        my_data_complete <- my_data |>
          filter(trust %in% 1:5,
                 threat %in% 1:5)





